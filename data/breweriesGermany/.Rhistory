### -------------------------
### Workshop: Web Scraping mit R - Eine kleine Einführung
### Simon Munzert
### Universität Konstanz
### -------------------------
### Vorbereitungen ----------
# Workspace aufräumen
rm(list=ls(all=TRUE))
# Pakete installieren und laden
pkgs <- c("RCurl", "XML", "stringr", "httr", "rvest", "plyr", "ggplot2", "ggmap")
# install.packages(pkgs)
lapply(pkgs, library, character.only=T)
# Arbeitspfad speichern
basepath <- getwd()
getwd()
wd <- ("data/phonebookNamemaps")
setwd(paste0(basepath, wd))
wd <- ("/data/phonebookNamemaps")
setwd(paste0(basepath, wd))
wd <- ("/data/breweriesGermany")
setwd(paste0(basepath, wd))
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
mapGermany <- get_map(location = 'Germany', zoom = 6)
breweryMap <-
ggmap(mapGermany) +
geom_point(data = pos,
aes(x = lon, y = lat),
colour = "#F54B1A70", size=2, na.rm=T)
breweryMap
url <- "http://www.biermap24.de/brauereiliste.php"
browseURL(url)
content <- html(url, encoding = "utf8")
anchors <- html_nodes(content, xpath = "//tr/td[2]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
cities
length(unique(cities))
sort(table(cities))
